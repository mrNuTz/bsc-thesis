\documentclass[../main.tex]{subfiles}
\begin{document}

What runtime codes do, is firstly they get external data through function arguments, blockchain data or calling of other contracts, secondly they prepare data on the stack, thirdly they use that data to cause an externally observable effect like storing, logging, transacting, self destruction, calling contracts and deploying contracts, and fourthly they repeat one, two and three.

Small changes in the codes can cause big changes in the digests---pre-processing aims to remove non-essential parts.

\subsection{Segmentation \label{sec:seg}}
Segmentation splits the codes into \code{code}, \code{data} and \code{meta} sections \cite{ethutils}.

\code{meta} sections have no effect on execution and change between compilations.

\code{data} sections are e.g. constructor arguments.
Constructor-arguments are processed by the deployment code.
Generally they are only used for parameter initialization and have no essential effect on the execution.
Limiting the possible effectiveness is the fact that detection is heuristic.

The first \code{code} section is the the actual contract, the other code-sections are in essence just data for the first-section, if it itself deploys further contracts.

\subsection{Skeletonization \label{sec:skel}}
Skeletons are runtime codes where constructor arguments, data sections, meta sections and push arguments are set to zero \cite{ethutils}.
Contracts can be associated via skeletons, because many deployed codes have identical skeletons.
Push-operations are the only EVM-instructions followed by data.
The reasoning behind this removal is that these data bytes have no essential effect on the execution, e.g. jump-addresses and Ethereum-addresses.
Setting the push arguments to zero has the benefit of preserving the ability to disassemble the code.

\subsection{Opcode filtering}
The same externally observable behavior can be achieved via different opcode sequences.
Some opcodes are more likely to change with solc-versions or compile-options than others.
Removing less significant opcodes before hashing should yield more meaningful similarity scores.

\subsubsection{\n{fStat} Filter \label{sec:fStat}}
Determining exactly which opcode sequences change, would most likely require a lot of manual work.
To quickly and simply distinguish between opcode which are likely to change and ones that do not change, we used the \n{\solcts} \secref{sec:solcts} and reduced the codes to \n{bytebags} \secref{sec:bytebag}. Then, we calculated an the \n{One-way ANOVA F-test statistic} \eqref{eq:ftest} for each opcode, using common source-code as grouping criterion and the count of the opcode as value.

To define the \n{fStat} filter, we selected the top 30 opcodes by f-statistic value, plus a few that never occurred or only in one group. See \secref{sec:runOpcodes} for the f-statistic values and more details.

\begin{equation}
  F = \dfrac{\n{between group variability}}{\n{within group variability}}
  \label{eq:ftest}
\end{equation}

\begin{lstlisting}[style=pymd]
OPCODES = (
  # top 30 fStat values
  ADDRESS, LOG3, TIMESTAMP, ORIGIN, LOG4, SHA3, SWAP14, CALLDATASIZE,
  CALLDATACOPY, SIGNEXTEND, CALL, LOG2, RETURNDATASIZE, CALLER, EXTCODESIZE,
  JUMPI, STATICCALL, RETURNDATACOPY, GAS, DUP13, DUP5, DUP8, GASPRICE,
  SHR, PUSH4, ISZERO, DUP7, ADD, DUP9, MUL,
  # occurred in only one group
  XOR, CALLVALUE, DELEGATECALL, SELFDESTRUCT,
  # never occurred
  SAR, LOG0, CREATE,
)
\end{lstlisting}

\end{document}
