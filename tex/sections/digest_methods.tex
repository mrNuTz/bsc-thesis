\documentclass[../main.tex]{subfiles}
\begin{document}

\subsection{Fourbytes - macro-similarity \label{sec:fourbyte}}

The interface of a contract is the set of functions callable by other accounts/contracts.
Almost all deployed contracts follow the ABI standard for encoding there interface, which lets the caller select the desired function via a four-byte hash of the function signature.
The macro-similarity based on the contract-interface is described in \n{II.C.2) Interface Restoration}\\\aycite{di2020characteristics}.

The digest produced by this method is the set of extracted fourbyte signatures. Similarity scores are obtained via the Jaccard-Index \secref{sec:jacc} off these sets.

\subsection{ssdeep \label{sec:ssdeep}}
\n{ssdeep} is a Context Triggered Piecewise Hash (CTPH) based on \kcite{spamsum} which was written for email spam detection. It is described in detail in the accompanying paper \aycite{kornblum2006identifying}

Context Triggered Piecewise Hashes follow the steps:
\begin{ol}
  \item Compute a rolling hash of the last n bytes for every position.
  \item The rolling hash is used to determine the cutoff points.
  \item The resulting chunks are hashed using a traditional cryptographic hash.
  \item The final hash results from concatenating part of the chunk-hashes (e.g. the last byte).
\end{ol}
\n{ssdeep} compares the hashes via a custom edit-distance and sores similarity from 0 to 100, we divide by 100 to get scores in [0,1].

\subsection{ppdeep \label{sec:ppdeep}}
To make modifications easy, we used a version of ssdeep implemented in pure Python \kcite{ppdeep}. The score calculation differs from the original implementation, but scatter plots \figref{fig:ssppScat} showed insignificant impact for the purposes of this work. \n{ppdeep} hashes are compared via Levenshtein distance \secref{sec:lev} and custom weighting to produce a similarity scores in [0,100], we divide by 100 to get scores in [0,1].

\subsection{ppdeep\_mod \label{sec:ppdeep_mod}}
The following modifications where made to \n{ppdeep}:
\begin{ul}
  \item Remove sequence-stripping. It made many hashes incomparable because of long strips of 'K' chunk-hashes.
  \item Remove rounding in the score calculation to differentiate between exact match and close match as well as incomparable and minor similarity.
  \item Remove common substring detection to make more hashes comparable.
  \item Handle case where first chunk is never triggered.
  \item Add option to use Jaccard-Index for comparison, the default is Levenshtein-similarity.
\end{ul}

\subsection{jumpHash \label{sec:jump}}
Inspired by ssdeep and the learnings from the \kcite{solc-versions-testset}, we implemented jumpHash \cite{ethereum-contract-similarity}, it follows the steps:
\begin{ol}
  \item Split the code by the opcode \code{JUMPI=0x57} into chunks.
  \item Hash each chunk with \code{sha1}.
  \item Map the first byte of the \code{sha1} hash to a Unicode character.
  \item Concatenate the Unicode characters to a hash-string.
  \item Compare the hash-strings via Levenshtein-similarity \secref{sec:lev}.
\end{ol}
\begin{lstlisting}[style=pymd]
from hashlib import sha1

def h(b: bytes) -> str:
  return chr(sha1(b).digest()[0] + 0xb0)

def hash(code: bytes) -> str:
  jumpi = b'\x57'
  chunks = code.split(jumpi)
  return ''.join(h(chunk) for chunk in chunks)
\end{lstlisting}
The Unicode character \code{Â°=chr(0xb0)} was chosen as \code{0} chunk-hash because it is followed by 255 valid characters.

\subsection{Bytebag - Opcode Frequency \label{sec:bytebag}}
As lower reference bound for more complex similarity detection, we implemented the following measure:

\begin{ol}
  \item Count every byte-value in the code forming a multiset or bag of byte-values, a bytebag.
  \item Compare via Jaccard-Index for bags (code below).
\end{ol}

\begin{lstlisting}[style=pymd]
def byteBag(code: bytes) -> Dict[int,int]:
  def reducer(counts: Dict[int,int], b: int):
    counts[b] = counts[b] + 1 if b in counts else 1
    return counts
  return functools.reduce(reducer, code, {})

def jaccard(a: Dict[int,int], b: Dict[int,int]) -> float:
  return sum(min(a[i], b[i]) for i in range(256)) /
    sum(max(a[i], b[i]) for i in range(256))
\end{lstlisting}

\subsection{LZJD - Lempel-Ziv Jaccard Distance \label{sec:lzjd}}
Designed as a fast approximation of the Normalized Compression Distance (\n{NCD}) \cite{raff2017alternative}, Lempel-Ziv Jaccard Distance (\n{LZJD}) can be used as alternative to \n{sdhash} and \n{ssdeep} \cite{raff2018lempel}.

The digest generated by \n{LZJD} (\n{LZSet}) is an LZ dictionary, which is a set of sub-sequences, defined by \n{Algorithm 1} in \cite{raff2017alternative}. Distance is calculated via the Jaccard-Index \secref{sec:jacc} on the \n{LZSets} \eqref{eq:lzjd}.

\begin{equation}
  \n{LZJD}(x,y) = 1 - J(\n{LZSet}(x), \n{LZSet}(y))
  \label{eq:lzjd}
\end{equation}

We used the \n{pyLZJD}\cite{raff2019pylzjd} implementation and calculated similarity instead of distance.

\subsection{bzHash \label{sec:bz}}
bzHash is based on peHash\cite{wicherski2009pehash} which calculates the compression ratio for each section of potentially malicious executables to generate a fingerprint. The digest is calculated via the following steps.

\begin{ol}
  \item Split code by \code{JUMPI} into chunks.
  \item For each chunk: calculate the bz-compression-ratio.
  \item Calculate mean and standard-deviation of the ratios.
  \item Calculate distance from mean as multiple of the standard-deviation.
  \item Clamp values to [-2, 2] standard-deviations from mean.
  \item Discretize to positive integer values.
  \item Map the integers to characters.
  \item Concatenate the characters to the final hash-string.
  \item Compare the hash-strings with Levenshtein \secref{sec:lev}.
\end{ol}

\begin{lstlisting}[style=pymd]
import numpy as np
import bz2

def bzCompRatio(code: bytes):
  return len(bz2.compress(code)) / len(code) if len(code) > 0 else 1

_map = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-_'

def bzHash(code:bytes, chunkRes=4) -> str:
  jumpi = b'\x57'
  res = [bzCompRatio(chunk) for chunk in code.split(jumpi)]
  res = sdFromMean(res)
  return ''.join(_map[discretize(-2, 2, chunkRes, val)] for val in res)

def discretize(mi, ma, resolution, val):
  span = ma - mi
  shifted = val - mi
  scaled = shifted * (resolution / span)
  return max(0, min(resolution - 1, int(scaled)))

def sdFromMean(x: Iterable) -> Iterable:
  x = np.fromiter(x, float)
  if len(x) == 0:
    return x
  elif x.std() == 0:
    return x - x
  return (x - x.mean()) / x.std()
\end{lstlisting}

\end{document}
