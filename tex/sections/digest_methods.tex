\documentclass[../main.tex]{subfiles}
\begin{document}

\todo{reference to exact page number or figure}

\subsection{Fourbytes - macro-similarity}

The interface of a contract is the set of functions callable by other accounts/contracts.
Almost all deployed contracts follow the ABI standard for encoding there interface, which lets the caller select the desired function via a four-byte hash of the function signature.
Contracts can be associated via interface by calculating the Jaccard Index on there sets of fourbyte signatures.

The macro-similarity based on the contract-interface is described in `II.C.2) Interface Restoration` \aycite{di2020characteristics}

\subsection{ssdeep - Context Triggered Piecewise Hashes (CTPH)}
ssdeep is based on spamsum\cite{spamsum} wich was written for email spam detection.

Context Triggered Piecewise Hashes follow the steps:
\begin{ol}
  \item Compute a rolling hash of the last n bytes for every position.
  \item The rolling hash is used to determine the cutoff points.
  \item The resulting chunks are hashed using a traditional cryptographic hash.
  \item The final hash results from concatenating part of the chunk-hashes (e.g. the last byte).
\end{ol}

\subsubsection{Modified ssdeep}
To make modifications easy I used a version of ssdeep implemented in pure python (ppdeep \cite{ppdeep}).

The following modifications where made:
\begin{ul}
  \item Remove sequence-stripping. It made many hashes incomparable because of long strips of 'K' chunk-hashes.
  \item Remove rounding in the score calculation to differentiate between exact match and close match as well as incomparable and minor similarity.
  \item Remove common substring detection to make more hashes comparable.
  \item Handle case where first chunk is never triggered.
  \item Add option to use Jaccard-Index for comparison, the default is Levenshtein-similarity.
\end{ul}

\subsection{jumpHash}
\begin{ol}
  \item Split code by \code{JUMPI} into chunks.
  \item Hash each chunk with \code{sha1}.
  \item Map the first byte to a Unicode character.
  \item Concatenate the Unicode characters to a hash-string.
  \item Compare the hash-strings via Levenshtein-similarity.
\end{ol}
\begin{lstlisting}[style=mystyle,language=Python]
from hashlib import sha1

def h(b: bytes) -> str:
  return chr(sha1(b).digest()[0] + 0xb0)

def hash(code: bytes) -> str:
  jumpi = b'\x57'
  chunks = code.split(jumpi)
  return ''.join(h(chunk) for chunk in chunks)
\end{lstlisting}

\subsection{Bytebag - Opcode Frequency}
As lower reference bound for more complex similarity detection I implemented the following measure:

\begin{ol}
  \item Count every byte-value in the code forming a multiset or bag of byte-values, a bytebag.
  \item Compare via Jaccard-Index for bags.
\end{ol}

\begin{lstlisting}[style=mystyle,language=Python]
def byteBag(code: bytes) -> Dict[int,int]:
  def reducer(counts: Dict[int,int], b: int):
    counts[b] = counts.get(b, 0) + 1
    return counts
  return functools.reduce(reducer, code, {})

def jaccard(a: Dict[int,int], b: Dict[int,int]) -> float:
  return sum(min(a[i], b[i]) for i in range(256)) /
    sum(max(a[i], b[i]) for i in range(256))
\end{lstlisting}

\subsection{LZJD - Lempel-Ziv Jaccard Distance}
LZ compression is done by generating a dictionary of frequent sequences, this dictionary is the result of this hashing method.

This is how a cite\cite{sung2004static} looks like.

\subsection{bzHash}
bzHash is based on peHash\cite{wicherski2009pehash} which calculates the compression ratio for each section of potentially malicious executables to generate a fingerprint.

\begin{ol}
  \item Split code by \code{JUMPI} into chunks.
  \item For each chunk: calculate bz-compression-ratio.
  \item Calculate mean and standard-deviation of the ratios.
  \item Calculate distance from mean as multiple of the standard-deviation.
  \item Clamp values to [-2, 2] standard-deviations.
  \item Discretize to positive integer values in [0,3].
  \item Map the integers to characters.
  \item Concatenate the characters to the final hash-string.
  \item Compare the hash-strings with Levenshtein.
\end{ol}

\begin{lstlisting}[style=mystyle,language=Python]
import numpy as np
import bz2

def bzCompRatio(bs: bytes):
  return len(bz2.compress(bs)) / len(bs) if len(bs) > 0 else 1

def bzHash(bs:bytes, chunkRes=4) -> str:
  jumpi = b'\x57'
  res = [bzCompRatio(chunk) for chunk in bs.split(jumpi)]
  res = sdFromMean(res)
  return ''.join(chr(0xb0 + discretize(-2, 2, chunkRes, val)) for val in res)

def discretize(mi, ma, resolution, val):
  span = ma - mi
  shifted = val - mi
  scaled = shifted * (resolution / span)
  return max(0, min(resolution - 1, int(scaled)))

def sdFromMean(x: Iterable) -> Iterable:
  x = np.fromiter(x, float)
  if len(x) == 0:
    return x
  elif x.std() == 0:
    return x - x
  return (x - x.mean()) / x.std()
\end{lstlisting}

\subsection{Normalized compression distance NCD}

NCD is a measure for how well two files co-compress.

The more features two files have in common the shorter the length of the compressed concatenation.

$Z(x)$ is the length of the compressed file $x$ ; $xy$ is the concatenation of $x$ and $y$.

\begin{equation}
  NCD(x,y) = \dfrac{Z(xy) - \min \{Z(x),Z(y)\}}{\max \{Z(x),Z(y)\}}
\end{equation}

I calculation the similarity instead of the distance to be consistent with the other measures.

\begin{lstlisting}[style=mystyle,language=Python]
import lzma

def Z(contents: bytes) -> int:
  return len(lzma.compress(contents, format=lzma.FORMAT_RAW))

def NCD(a: bytes, b: bytes):
  return (Z(a + b) - min(Z(a), Z(b))) / max(Z(a), Z(b))

def similarity(a: bytes, b: bytes):
  return (Z(a) + Z(b) - Z(a + b)) / max(Z(a), Z(b))
\end{lstlisting}

\end{document}
